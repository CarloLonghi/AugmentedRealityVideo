{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Put the code in try-except statements catch the keyboard exception and release the camera device and \n",
    "# continue with the rest of code.\n",
    "def play_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # Try-except statement to manage exceptions\n",
    "    try:\n",
    "        while(True):\n",
    "            # Capture frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame is None:\n",
    "                # Release the Video if ret is false\n",
    "                cap.release()\n",
    "                print(\"Released Video Resource\")\n",
    "                # Break exit the for loops\n",
    "                break\n",
    "            \n",
    "            # Display the frame\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.axis('off')\n",
    "            plt.title(\"Input Stream\")\n",
    "            plt.imshow(frame)\n",
    "            plt.show()\n",
    "            \n",
    "            # Clear cell output when new frame is available\n",
    "            clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        # If we press stop (jupyter GUI) release the video\n",
    "        cap.release()\n",
    "        print(\"Released Video Resource\")\n",
    "\n",
    "\n",
    "video = cv2.VideoCapture('res/Multiple View.avi')\n",
    "w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, fps, (w,  h))\n",
    "\n",
    "img_test = cv2.imread('res/ReferenceFrame.png')\n",
    "img_over = cv2.imread('res/AugmentedLayer.PNG')\n",
    "over_mask = cv2.imread('res/AugmentedLayerMask.PNG',0)\n",
    "reference_mask = cv2.imread('res/ObjectMask.PNG',0)\n",
    "\n",
    "img_over = img_over[:,:640]\n",
    "over_mask = over_mask[:,:640]\n",
    "\n",
    "o_height, o_width = img_over.shape[:2]\n",
    "t_height, t_width = img_test.shape[:2]\n",
    "\n",
    "print(img_test.shape)\n",
    "print(img_over.shape) \n",
    "\n",
    "# apply the augmented image over the original frame\n",
    "inv_over_mask = cv2.bitwise_not(over_mask)\n",
    "img_test = cv2.bitwise_and(img_test,img_test,mask=inv_over_mask)\n",
    "img_over = cv2.bitwise_and(img_over,img_over,mask=over_mask)\n",
    "img_test = cv2.add(img_test,img_over)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_test, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# now work on every frame of the video\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "###############\n",
    "# Defining index for approximate kdtree algorithm\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "\n",
    "# Defining parameters for algorithm \n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "\n",
    "# Defining search params.\n",
    "# checks=50 specifies the number of times the trees in the index should be recursively traversed.\n",
    "# Higher values gives better precision, but also takes more time\n",
    "search_params = dict(checks = 50)\n",
    "\n",
    "# Initializing matcher\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "###############\n",
    "\n",
    "# find keypoints in the reference frame\n",
    "kp_reference = sift.detect(img_test,reference_mask)\n",
    "\n",
    "# compute the descriptors of the reference frame\n",
    "kp_reference, des_reference = sift.compute(img_test, kp_reference)\n",
    "\n",
    "try:\n",
    "    while(True):\n",
    "        # Capture frame\n",
    "        ret, frame = video.read()\n",
    "        if not ret or frame is None:\n",
    "            # Release the Video if ret is false\n",
    "            video.release()\n",
    "            print(\"Released Video Resource\")\n",
    "            # Break exit the for loops\n",
    "            break\n",
    "        \n",
    "        # find keypoints\n",
    "        kp_new = sift.detect(frame)\n",
    "        # img_visualization = cv2.drawKeypoints(frame,kp_new,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        # img_visualization = cv2.cvtColor(img_visualization,cv2.COLOR_BGR2RGB)\n",
    "        # plt.imshow(img_visualization)\n",
    "        # plt.show()\n",
    "        \n",
    "        # compute the descriptors\n",
    "        kp_new, des_new = sift.compute(frame, kp_new)\n",
    "            \n",
    "        # feature matching with the reference frame\n",
    "        matches = flann.knnMatch(des_reference,des_new,k=2)\n",
    "        \n",
    "        # filter bad matches\n",
    "        good = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < 0.7*n.distance:\n",
    "                good.append(m)\n",
    "        \n",
    "        ### calculate the homography\n",
    "        MIN_MATCH_COUNT = 10\n",
    "        if len(good)>MIN_MATCH_COUNT:\n",
    "            # building the corrspondences arrays of good matches\n",
    "            src_pts = np.float32([ kp_reference[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "            dst_pts = np.float32([ kp_new[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "            # Using RANSAC to estimate a robust homography. \n",
    "            # It returns the homography M and a mask for the discarded points\n",
    "            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        else:\n",
    "            print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\n",
    "        \n",
    "        ### make the transformation of the augmented image\n",
    "        augmented_new = cv2.warpPerspective(img_over, M, (t_width,t_height))\n",
    "        mask_new = cv2.warpPerspective(over_mask, M, (o_width,o_height))\n",
    "       \n",
    "    \n",
    "        # apply the augmented image over the original frame\n",
    "        inv_over_mask = cv2.bitwise_not(mask_new)\n",
    "        frame = cv2.bitwise_and(frame,frame,mask=inv_over_mask)\n",
    "        augmented_new = cv2.bitwise_and(augmented_new,augmented_new,mask=mask_new)\n",
    "        frame = cv2.add(frame,augmented_new)\n",
    "        \n",
    "        # write the video\n",
    "        #frame = np.flip(frame, axis=0)\n",
    "        out.write(frame)\n",
    "        \n",
    "        # Display the frame\n",
    "        #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        #plt.axis('off')\n",
    "        #plt.title(\"Input Stream\")\n",
    "        #plt.imshow(frame)\n",
    "        #plt.show()\n",
    "            \n",
    "        # Clear cell output when new frame is available\n",
    "        clear_output(wait=True)\n",
    "except KeyboardInterrupt:\n",
    "    # If we press stop (jupyter GUI) release the video\n",
    "    video.release()\n",
    "    print(\"Released Video Resource\")\n",
    "\n",
    "video.release()\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('CDMOEnvironment': conda)",
   "language": "python",
   "name": "python3810jvsc74a57bd0a85bfb77db1f35976007f0f76c6861d275c57c0b832739ae05da8ce4d924aa08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
